---
date: 2017-12-3 17:49
status: public
title: '[笔记]Playing Atari with Deep Reinforcement Learning'
layout: post
tag: [机器学习, 强化学习]
categories: 
description: 
---

* 目录 
{:toc}

# 背景

很多人都有着同样一个梦想：在未来人类的生活中有着很多通用人工智能，能帮助我们处理琐碎事情或者充当我们生活中不可或缺的角色。这样的一个角色目前只存在于科幻小说或者电影中，但是这并不妨碍我们一步步朝着这个方向努力。

首先我们可以幻想一下，如果这样的人工智能出现，他应该具备什么样的特质或者是能力。我们以自动驾驶的无人车为例：
1. 作为一个无人车首先应该清楚路面的状况，这就需要他能看清道路，并且分析出当前的路况，比如路边的行人在哪，路口还有多远，前后方车子距离多少等信息。也就是说能够通过当前看到的画面提取出一些有用的信息。
2. 无人车还应该清楚如果到达路口遇到红灯，这时候就该等红灯过了才能继续前进。能够针对特定的状况执行对应的操作。
3. 更高阶一点的无人车应该具备实时分析路况的能力，联想我们平常开车的时候，如果有两条路线可以选择，往往我们选择的不是最短的那条路，而是到的最快的或者是开起来舒坦的。具备一定的规划能力和推理能力。

除此之外当然还有其他的一些能力没有列举，上面三点是可以通过强化学习方式处理的一些问题，现在我们来想想一般的强化学习的问题是怎样的。

# 强化学习基本模型

还是针对这个无人车的问题，无人车是我们的AI载体，起着做决定的作用，也是我们着力打造的人工智能，一般的强化学习问题中都有着这样一个智能体，他会做决策，重要的是能够通过一次次的经验学习，不断地完善自己的决定，让决定的时候少犯错误，我们一般将这样的智能体称作是**Agent**。

而无人车周边的事物，包括路边的行人，路上的标识以及路口的红路灯，还有无人车本身，都在一个大的环境中，这些所有与之相关的事物所在的这个环境我们称之为**Environment**。

无人车在路上跑的时候要时时刻刻关注着路上的情况，就像是人类在开车的时候获取到的所有的信息，无人车每次都是基于这些信息判断这步改采取什么动作，一般把当前时刻看到的所有信息汇聚起来，传给无人车的大脑做出决策，然后无人车执行对应的操作。传递给大脑进行决策的信息我们称作为**State**，做出的相应动作称为**Action**。

由于无人车不是一开始就会自己开车，从新手到熟练开车有一个学习的过程，就像平常我们学习也会通过考试得分的高低判断自己学的好不好，得分高就继续保持，得分低就修改这个动作。在无人车的学习过程中我们也会设定一个这样的得分，比如在一段时间里车子开得平稳坐着舒服我们就给他打一分，让他知道这段时间的操作应该要好好保持，下次遇到类似的情况也要这么开车。如果有一段时间车子在不停的左右晃，甚至在路口不减速，或者与前面的车子距离太近导致大家坐车心惊胆战，我们也许就会给这段时间打-1分，告诉他这样开车很危险，以后不能这么开车了。前面的一分和负一分我们往往称为**Reward**。无人车往往会参考这个reward进行自身的学习，不断提高自己的驾驶技术。

# 让AI玩Atari游戏

前面说到的前景十分美好，但是一口气直接实现也是有些困难，所以我们不妨通过让AI自己解决一些特定的问题开始。一个特定的问题需要这个问题比较简单，并且符合我们上面说到的基本模型，还有比较客观的得分。AI先解决了这个问题我们才能清晰的思考能否把这个经验用在更复杂一点的问题上，攻克了一个个的困难之后，我们距离通用人工智能也就不太遥远了。

所以我们要找到一个比较良好的实验环境，让我们的AI能在这个环境中不断地学习，DeepMind团队为我们想到了一个很完美的环境就是Atari游戏。可能大家对这个名字比较陌生，但是大家一定不会对下面这几款游戏画面感到陌生：

![games](http://7xrop1.com1.z0.glb.clouddn.com/paper/games.png)

之所以说这些游戏的环境很完美，首先这些游戏比较简单，AI能选择的动作一般只有几个（还记得当年游戏机上的手柄按键嘛，最多只有18中不同可能）并且整个游戏环境也不太复杂，每一帧画面的大小是 $ 210 * 160 $ 像素，每个像素又只有128种可能（Atari游戏只有128种不同颜色）。更重要的是，游戏往往有着非常客观的评分系统，赢了就设置为1分，输了就设置为-1分。由于这些特性，Atari游戏对我们研究AI是非常简便并且有帮助的。

# 基本思路

明确了整个实验的环境，我们可以考虑下怎么实作这个AI。在之前已经有了Qlearning的方法，但是这个方法几乎没有办法应用在这些游戏中，原有的Qlearning是列出一张表格，记录了各种state中每个action的估值。但是对于Atari的游戏画面，一个state可能的情况有 $ (210 * 160)^{128} $ 种，几乎是不可能存在一张表中，就算存进去了也很难学习到具体的内容。因为一个状态到达的次数特别少，很难在学习阶段通过有限的迭代计算出Q表中的值。

因此DeepMind在[这篇paper](https://arxiv.org/pdf/1312.5602.pdf)中提到了一种新的方式，既然由于有些状态比较相近，可以采取类似的策略，那么能否将CNN与传统的RL结合来处理这个问题，直接将当前的state作为一个神经网络的输入，让这个网络给出这个状态下每个动作对应的估值，然后根据这个估值来进行决策过程，在学习阶段也是不断地迭代这个网络的参数来达到更好的效果。简单来讲就是用一个网络来代替了我们之前查表的过程，这个输入与输出都是相同的。（输入是当前状态，输出是每个动作对应的估值）:

![20171204193129](http://7xrop1.com1.z0.glb.clouddn.com/paper/20171204193129.jpg)

回顾一下整体的流程，一开始我们有一个神经网络，能接收输入的状态，输出这个状态下所有动作的估值，所以游戏的每个状态我们都会得到这个状态下所有动作的估值，选取最好的动作（估值最大）。每个状态都进行这样的操作，这就能保证我们可以顺利的完成一局游戏。

接下来就是要想办法保证我们的网络给出的结果足够准确。还记得上面提到的无人车学习怎么驾驶的情况吗，这边玩Atari游戏也是类似，只不过这里不仅仅是通过reward来评价一个动作的好坏，这边还加上了下一个状态的估值，这就防止了落入陷阱中这类问题（落入陷阱之前的reward一般比较高，但是下一个状态往往就会直接挂掉并且不可逆转，这里就会通过给下一个状态打一个很低的估值来避免这个情况）。用下一个状态的估值和当前的reward来评价当前动作的好坏。

# 公式推导

上面简单说明了这个方法的大概思路，这里需要从理论上证明这个学习的过程是可以收敛的。

首先，DeepMind做出了第一个假设，这个游戏的过程是一个**马尔科夫决策过程（MDP）：下一个状态仅仅取决于当前的状态和当前的动作**。这个假设就将过去剔除了出去，当前的Q值不再取决于过去发生了什么，也不取决于是从哪个状态通过哪个动作到达的现在的状态。因此我们可以假设目前状态在未来的影响( $ R_t $ )是未来所有状态下reward的总和：

$$
R_t = r_t + r_{t+1} + r_{t+2} + ... + r_T
$$

但是根据经验，不同时间的选择其实对未来造成的影响是不一样的，时间越远造成的影响会越小。换句话说现在的回报不应该表示成未来所有reward的总和，而应该是在未来的reward上乘以一个衰减系数。所以上面的公式应该进行一些修改：

$$
R_t = r_t + \gamma * r_{t+1} + {\gamma}^2 * r_{t+2} + ... + {\gamma}^{T-t} * r_T
$$

对这个公式进行一些改动：

$$
\begin{align}
R_t & = r_t + \gamma * r_{t+1} + {\gamma}^2 * r_{t+2} + ... + {\gamma}^{T-t} * r_T \\
 & = r_t + \gamma * (r_{t+1} + {\gamma} * r_{(t+1)+1} + ... + {\gamma}^{T-(t+1)} * r_T)
\end{align}
$$

到了这一步，DeepMind做出了另一个重要的假设：每个时刻在未来的回报 $ R_t $ 满足**Bellman Equation：在一个序列求解的过程中，如果一个解的路径是最优路径，那么其中的每个片段都是当前的最佳路径**。

简单来讲就是如果我们希望t时刻在未来的影响达到最好，在t时刻做出选择之后，t+1时刻在未来的影响也要最好。

把理想状态下对未来最好的影响记做 $ Q^* $， $ Q^* $满足下面公式：

$$
\begin{align}
Q^*(s_t, a_t) & = R_t \\
 & = r_t + \gamma * (r_{t+1} + {\gamma} * r_{(t+1)+1} + ... + {\gamma}^{T-(t+1)} * r_T)
\end{align}
$$

根据Bellman Equation，在 $ s_{t+1} $状态下，需要使Q值最大的动作才能达到 $ Q^*(s_t, a_t) $ ，所以进一步整理公式：

$$
\begin{align}
Q^*(s_t, a_t) & = r_t + \gamma * (r_{t+1} + {\gamma} * r_{(t+1)+1} + ... + {\gamma}^{T-(t+1)} * r_T) \\
 & = r_t + \gamma * max_{a'} Q(s_{t+1}, a')
\end{align}
$$

$ max_{a'} Q(s_{t+1}, a') $ 表示状态 $ s_{t+1} $ 下所有动作中最大的Q值。

至此我们就可以根据下一个状态推导出现有状态理想情况下的Q值了。现在我们来整理一下模型参数的迭代过程：目前我们有一个状态 $ s_t $，将这个状态喂给模型得到了每个动作下对应的 $ Q_t $ 值，我们选取了Q值最大的动作得到了reward和下一个状态 $ s_{t+1} $。有了下一个state我们又可以通过模型得到所有动作对应的 
$ Q_{t+1} $
值。由上面公式可以得到
$Q_t^{\star} = r_t + \gamma * Q_{t+1}$
，理论上
$Q_t^{\star} = Q_t$
，但是由于模型一开始并不是完美的，得到的Q值也会跟理想情况有差距，所以才需要不停的迭代。我们可以用实际值和理想值之间的差值作为loss function，并通过大量样本最小化这个loss function来完善模型。

# 一些实作技巧

## Experience Reply

在学习阶段比较自然能想到的方式是在每次执行动作之后，就直接拿后续的reward和新的state估值来更新之前的这个state。但是DeepMind想到了一种更好的方式：先把每次动作后得到的 {s, r, s'}这样的组合放到一个容器里面，然后学习阶段随机从中间抽取一部分拿出来学习。他们发现这种方式比前面那种方法效果更好，分析了原因发现后面的这个方式有以下几种优势：
- 每一步的经验可能在更新神经网络权重的时候被使用很多次，让数据使用更有效率。
- 由于连续的样本之间有很强的关联性，使用连续的样本比较没有效率。而随机的样本能打破这种关联
- 如果不使用experience replay而采用on-policy的机制，当前的参数就决定了下一个训练样本，我们又要根据这个样本训练我们的参数，这样很容易导致训练过程中不愿意看到的反馈回路(feedback loop)，结果可能导致收敛到了一个局部最优解甚至是结果产生灾难性的偏移。

## frame-skipping technique

另一个小技巧是回想以下我们人类的观察和做决策的过程，我们玩游戏的时候并不是时时刻刻盯着每一帧画面观察，然后每一帧画面都做一个对应的动作，并且人类的反应速度也没有办法达到这么快。但是在这个条件下人类仍然玩Atari游戏玩的很好，所以理论上来讲有很多帧的画面是没有什么作用的。

DeepMind针对这个情况提出了我们不需要每帧画面都做出动作，也不需要每帧画面都执行对应的动作，所以他们提出可以没k帧画面观察一次，对于大部分游戏k=4，也就是说大部分游戏都是4帧画面观察一下游戏画面。

## 图像预处理 && 选择网络的输入

由于神经网络中会对输入进行卷积的操作，一般要求输入的图像是正方形，所以需要对图像进行预处理，原始画面是210 * 160像素，需要进行剪裁缩放变成84 * 84的大小，并且由彩色图像变为黑白图像。

此外，我们平常在玩游戏的过程中并不是看到每个画面都会执行动作，而是在几个画面之后才执行一个动作，DeepMind做出的AI玩Atari游戏也是一样，他们统一四个画面才执行一个动作。把4个连续画面整合成一个state喂给神经网络，输出所有动作的估值。

注意这边的4个连续画面和上面的frame-skipping不一样，就有点像游戏跑了16帧，由于frame-skipping的关系只能看到4帧的画面，但是这四帧画面是一起喂给神经网络的，也就是说实际上这16帧画面只执行了一个动作，其他的三个观察到的画面都延续之前的动作。

![20171204191416](http://7xrop1.com1.z0.glb.clouddn.com/paper/20171204191416.jpg)

# 实作结果

有了上面的知识储备差不多可以看出伪代码了：

![DeepMind_Deep_Reinforcement_Learning](http://7xrop1.com1.z0.glb.clouddn.com/others/DeepMind_Deep_Reinforcement_Learning.jpg)

以及神经网络的设计如下图：

![20171210032258](http://7xrop1.com1.z0.glb.clouddn.com/paper/20171210032258.png)

## 结果分析

不同于监督学习，强化学习并没有一个很好的评量标准，原本DeepMind打算采用不同训练时间下平均得分作为衡量标准，但是发现平均得分的震荡幅度有点大（如下图中最左边的两个图），分析原因推测是小的权重变化会导致选择动作的策略不同，策略的改变导致游戏过程中遇到的状态的分布有很大的变化，所以导致得分不稳定。因此最后选择平均Q值作为评估标准，下图中右边两幅图中也可以看出Q值比较平滑的上升。这表明虽然缺乏理论的证明，但是这种做法确实能用强化学习的信号训练大型的神经网络并且收敛。

![Playing_Atari_with_Deep_Reinforcement_Learning_1](http://7xrop1.com1.z0.glb.clouddn.com/paper/Playing_Atari_with_Deep_Reinforcement_Learning_1.png)

## Q值可视化

下图中的实验简单描述了Q值直观上的意义。游戏截图中的第一幅图表明左侧有一个新的敌人出现，这时候网络给出的估值瞬间飙高；第二幅图表示玩家子弹快要打中敌人了，如果打中就会产生一个新的reward，这时候的估值再次升高；第三幅图是子弹打到敌人已经拿到reward后，网络给出的估值又回到了原先敌人没有出现的地方附近。

![Playing_Atari_with_Deep_Reinforcement_Learning_2](http://7xrop1.com1.z0.glb.clouddn.com/paper/Playing_Atari_with_Deep_Reinforcement_Learning_2.png)

## 评估

此前也有一些算法在尝试解Atari的问题，其中一个叫做Sarsa方法使用Sarsa算法学习手动提取的特征，Contingency方法与Sarsa方法类似，但是有部分的学习过程。这两个方法都在手动提取特征的同时分离出了背景（有点类似把128种颜色作为128个图层分开，然后标注每种颜色代表什么），虽然对于Atari游戏，通常不同的颜色对应不同类别的物体，这种方式效果还不错。但是作为对比，DeepMind没有采用这个方式，而是需要神经网络自己学习分离背景和游戏物体。

此外，他们还比较了人类玩家和不同算法的得分情况。除了上面介绍的算法之外还有随机的选择动作。对比结果在下面列表所示，其中HNeat Best代表人工标注屏幕上物体的位置和类别的结果，HNeat Pixel代表使用8个特别的颜色做出八个图层，然后标注每个图层代表的类别的结果。

![Playing_Atari_with_Deep_Reinforcement_Learning_3](http://7xrop1.com1.z0.glb.clouddn.com/paper/Playing_Atari_with_Deep_Reinforcement_Learning_3.png)

# 参考

1. [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/pdf/1312.5602.pdf)
2. [List of video game console palettes](https://en.wikipedia.org/wiki/List_of_video_game_console_palettes#Atari_2600)
3. [强人工智能 - 维基百科，自由的百科全书](https://zh.wikipedia.org/wiki/%E5%BC%B7%E4%BA%BA%E5%B7%A5%E6%99%BA%E6%85%A7)
4. [马尔可夫性质 - 维基百科，自由的百科全书](https://zh.wikipedia.org/wiki/%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%80%A7%E8%B4%A8)
5. [Stanford University CS231n: Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/neural-networks-3/)
6. [Paper Reading 1 - Playing Atari with Deep Reinforcement Learning - songrotek的专栏 - 博客频道 - CSDN.NET](http://blog.csdn.net/songrotek/article/details/50581011)
7. [What is the Best Multi-Stage Architecture for object Recognition](http://yann.lecun.com/exdb/publis/pdf/jarrett-iccv-09.pdf)
